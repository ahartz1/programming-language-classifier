{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from lib.import_lang_data import *\n",
    "from lib.lang_featurizers import *\n",
    "# import glob\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gcc           58\n",
       "scala         43\n",
       "csharp        41\n",
       "yarv          39\n",
       "clojure       38\n",
       "python3       36\n",
       "ocaml         35\n",
       "jruby         34\n",
       "perl          34\n",
       "sbcl          34\n",
       "php           29\n",
       "racket        29\n",
       "hack          26\n",
       "javascript    25\n",
       "c              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data into lang_data, lang_results\n",
    "languages = ['gcc', 'c', 'csharp', 'sbcl', 'clojure', 'ghc' 'java', 'javascript',\n",
    "             'ocaml', 'perl', 'php', 'hack', 'py', 'python3', 'jruby', 'yarv', 'rb',\n",
    "             'scala', 'racket']\n",
    "lang_data, lang_results = read_polyglot(languages)\n",
    "lang_info = pd.DataFrame(lang_results)\n",
    "lang_info[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ruby           73\n",
       "C              59\n",
       "PHP            55\n",
       "Scala          43\n",
       "C#             41\n",
       "Clojure        38\n",
       "Python         36\n",
       "OCaml          35\n",
       "Common Lisp    34\n",
       "Perl           34\n",
       "Scheme         29\n",
       "JavaScript     25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_info = match_extensions(lang_info)\n",
    "lang_results = list(lang_info[0])\n",
    "lang_info[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80158730158730163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_featurizer = make_union(\n",
    "    BagOfWordsFeaturizer(20),\n",
    "    FunctionFeaturizer(num_nil,\n",
    "                       num_nil_caps,\n",
    "                       num_null,\n",
    "                       num_none,\n",
    "                       num_start_double_semicolons,\n",
    "                       num_start_hashes,\n",
    "                       num_bar_hash,\n",
    "                       percentage_of_punctuation)\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(lang_data, lang_results)\n",
    "\n",
    "pipe = make_pipeline(lang_featurizer, DecisionTreeClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       0.56      0.75      0.64        12\n",
      "         C#       1.00      0.56      0.71        18\n",
      "    Clojure       0.86      0.86      0.86         7\n",
      "Common Lisp       0.88      1.00      0.93         7\n",
      " JavaScript       0.60      0.60      0.60         5\n",
      "      OCaml       1.00      1.00      1.00        10\n",
      "        PHP       0.86      0.71      0.77        17\n",
      "       Perl       1.00      0.71      0.83         7\n",
      "     Python       0.83      1.00      0.91        10\n",
      "       Ruby       1.00      0.95      0.97        19\n",
      "      Scala       0.36      0.62      0.45         8\n",
      "     Scheme       0.86      1.00      0.92         6\n",
      "\n",
      "avg / total       0.85      0.80      0.81       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pipe.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to test with the assignment tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_tests():\n",
    "    X = []\n",
    "    y = None\n",
    "    files = glob.glob('test/*')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            X.append(f.read())\n",
    "\n",
    "    with open('test.csv') as f:\n",
    "        y = f.read()\n",
    "    \n",
    "    inter = y.split('\\n')\n",
    "    \n",
    "    y_out = []\n",
    "    for pair in inter:\n",
    "        y_out.append(pair.split(','))\n",
    "    \n",
    "    y_out = list(pd.DataFrame(y_out).pop(1))[:-1]\n",
    "    \n",
    "    y_df = match_extensions(pd.DataFrame(y_out))\n",
    "    y_out = list(y_df[0])\n",
    "    \n",
    "    return X, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_assignment_test, y_assignment_test = read_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_assignment_test, y_assignment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       0.00      0.00      0.00         1\n",
      "         C#       0.00      0.00      0.00         9\n",
      "    Clojure       0.00      0.00      0.00         0\n",
      "    Haskell       0.00      0.00      0.00         0\n",
      "       Java       0.00      0.00      0.00         0\n",
      " JavaScript       0.25      0.08      0.12        12\n",
      "      OCaml       0.00      0.00      0.00         4\n",
      "        PHP       0.00      0.00      0.00         4\n",
      "     Python       0.00      0.00      0.00         0\n",
      "       Ruby       0.00      0.00      0.00         0\n",
      "      Scala       0.00      0.00      0.00         2\n",
      "     Scheme       0.00      0.00      0.00         0\n",
      "        TCL       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.09      0.03      0.05        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahartz1/TIY/programming-language-classifier/.direnv/python-3.4.3/lib/python3.4/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ahartz1/TIY/programming-language-classifier/.direnv/python-3.4.3/lib/python3.4/site-packages/sklearn/metrics/classification.py:960: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pipe.predict(X_assignment_test), y_assignment_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
