{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from lib.import_lang_data import *\n",
    "from lib.lang_featurizers import *\n",
    "# import glob\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gcc           58\n",
       "scala         43\n",
       "csharp        41\n",
       "yarv          39\n",
       "clojure       38\n",
       "python3       36\n",
       "ocaml         35\n",
       "perl          34\n",
       "sbcl          34\n",
       "jruby         34\n",
       "php           29\n",
       "racket        29\n",
       "hack          26\n",
       "javascript    25\n",
       "c              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data into lang_data, lang_results\n",
    "languages = ['gcc', 'c', 'csharp', 'sbcl', 'clojure', 'ghc' 'java', 'javascript',\n",
    "             'ocaml', 'perl', 'php', 'hack', 'py', 'python3', 'jruby', 'yarv', 'rb',\n",
    "             'scala', 'racket']\n",
    "lang_data, lang_results = read_polyglot(languages)\n",
    "lang_info = pd.DataFrame(lang_results)\n",
    "lang_info[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C              59\n",
       "PHP            55\n",
       "Scala          43\n",
       "C#             41\n",
       "Ruby           39\n",
       "Clojure        38\n",
       "Python         36\n",
       "OCaml          35\n",
       "jRuby          34\n",
       "Common Lisp    34\n",
       "Perl           34\n",
       "Scheme         29\n",
       "JavaScript     25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_info = match_extensions(lang_info)\n",
    "lang_results = list(lang_info[0])\n",
    "lang_info[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65873015873015872"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_featurizer = make_union(\n",
    "    BagOfWordsFeaturizer(20),\n",
    "    FunctionFeaturizer(num_nil,\n",
    "                       percentage_of_punctuation)\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(lang_data, lang_results)\n",
    "\n",
    "pipe = make_pipeline(lang_featurizer, DecisionTreeClassifier())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       0.63      0.71      0.67        17\n",
      "         C#       0.73      0.57      0.64        14\n",
      "    Clojure       0.83      0.56      0.67         9\n",
      "Common Lisp       0.57      0.80      0.67         5\n",
      " JavaScript       0.67      0.57      0.62         7\n",
      "      OCaml       1.00      0.67      0.80         9\n",
      "        PHP       0.83      0.67      0.74        15\n",
      "       Perl       0.38      0.71      0.50         7\n",
      "     Python       1.00      1.00      1.00         7\n",
      "       Ruby       0.38      0.56      0.45         9\n",
      "      Scala       1.00      1.00      1.00         8\n",
      "     Scheme       0.67      0.80      0.73        10\n",
      "      jRuby       0.17      0.11      0.13         9\n",
      "\n",
      "avg / total       0.69      0.66      0.66       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pipe.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to test with the assignment tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_tests():\n",
    "    X = []\n",
    "    y = None\n",
    "    files = glob.glob('test/*')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            X.append(f.read())\n",
    "\n",
    "    with open('test.csv') as f:\n",
    "        y = f.read()\n",
    "    \n",
    "    inter = y.split('\\n')\n",
    "    \n",
    "    y_out = []\n",
    "    for pair in inter:\n",
    "        y_out.append(pair.split(','))\n",
    "    \n",
    "    y_out = list(pd.DataFrame(y_out).pop(1))[:-1]\n",
    "    \n",
    "    y_df = match_extensions(pd.DataFrame(y_out))\n",
    "    y_out = list(y_df[0])\n",
    "    \n",
    "    return X, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_assignment_test, y_assignment_test = read_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_assignment_test, y_assignment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       0.00      0.00      0.00        11\n",
      "    Clojure       0.00      0.00      0.00         1\n",
      "Common Lisp       0.00      0.00      0.00         4\n",
      "    Haskell       0.00      0.00      0.00         0\n",
      "       Java       0.00      0.00      0.00         0\n",
      " JavaScript       0.00      0.00      0.00         0\n",
      "      OCaml       0.00      0.00      0.00         1\n",
      "        PHP       0.00      0.00      0.00         0\n",
      "       Perl       0.00      0.00      0.00         5\n",
      "     Python       0.50      0.25      0.33         8\n",
      "       Ruby       0.00      0.00      0.00         2\n",
      "      Scala       0.00      0.00      0.00         0\n",
      "     Scheme       0.00      0.00      0.00         0\n",
      "        TCL       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.12      0.06      0.08        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahartz1/TIY/programming-language-classifier/.direnv/python-3.4.3/lib/python3.4/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ahartz1/TIY/programming-language-classifier/.direnv/python-3.4.3/lib/python3.4/site-packages/sklearn/metrics/classification.py:960: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pipe.predict(X_assignment_test), y_assignment_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
