{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lib.import_lang_data import *\n",
    "from lib.lang_featurizers import *\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gcc           58\n",
       "scala         43\n",
       "csharp        41\n",
       "yarv          39\n",
       "clojure       38\n",
       "python3       36\n",
       "ocaml         35\n",
       "perl          34\n",
       "sbcl          34\n",
       "jruby         34\n",
       "racket        29\n",
       "php           29\n",
       "hack          26\n",
       "javascript    25\n",
       "c              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in assignment's test files\n",
    "X_assignment_test, y_assignment_test = read_tests()\n",
    "\n",
    "# read corpus into lang_data, lang_results\n",
    "languages = ['gcc', 'c', 'csharp', 'sbcl', 'clojure', 'ghc' 'java', 'javascript',\n",
    "             'ocaml', 'perl', 'php', 'hack', 'py', 'python3', 'jruby', 'yarv', 'rb',\n",
    "             'scala', 'racket']\n",
    "lang_data, lang_results = read_polyglot(languages)\n",
    "lang_info = pd.DataFrame(lang_results)\n",
    "lang_info[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ruby           73\n",
       "C              59\n",
       "PHP            55\n",
       "Scala          43\n",
       "C#             41\n",
       "Clojure        38\n",
       "Python         36\n",
       "OCaml          35\n",
       "Perl           34\n",
       "Common Lisp    34\n",
       "Scheme         29\n",
       "JavaScript     25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_info = match_extensions(lang_info)\n",
    "lang_results = list(lang_info[0])\n",
    "lang_info[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.9444444444444444\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          C       1.00      1.00      1.00        15\n",
      "         C#       1.00      1.00      1.00         8\n",
      "    Clojure       1.00      0.92      0.96        13\n",
      "Common Lisp       0.92      1.00      0.96        11\n",
      " JavaScript       0.78      1.00      0.88         7\n",
      "      OCaml       0.88      0.88      0.88         8\n",
      "        PHP       1.00      0.73      0.85        15\n",
      "       Perl       0.88      1.00      0.93         7\n",
      "     Python       1.00      0.89      0.94         9\n",
      "       Ruby       1.00      1.00      1.00        12\n",
      "      Scala       0.94      1.00      0.97        16\n",
      "     Scheme       0.83      1.00      0.91         5\n",
      "\n",
      "avg / total       0.95      0.94      0.94       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_featurizer = make_union(\n",
    "    BagOfWordsFeaturizer(348),\n",
    "    FunctionFeaturizer(presence_nil,\n",
    "                       presence_nil_caps,\n",
    "                       presence_null,\n",
    "                       presence_none,\n",
    "                       presence_start_double_semicolons,\n",
    "                       presence_start_hashes,\n",
    "                       presence_bar_hash,\n",
    "                       percent_start_and_end_parenthesis,\n",
    "                       presence_void,\n",
    "                       presence_public,\n",
    "                       presence_bool,\n",
    "                       presence_module_line,\n",
    "                       presence_extend_line,\n",
    "                       presence_require_line,\n",
    "                       presence_end,\n",
    "                       presence_multiple_end,\n",
    "                       presence_def_no_colon,\n",
    "                       presence_at,\n",
    "                       presence_double_at,\n",
    "                       presence_defn,\n",
    "                       percent_consecutive_closing_paren,\n",
    "                       presence_from_import_line,\n",
    "                       presence_import_line,\n",
    "                       presence_print,\n",
    "                       presence_dot_join,\n",
    "                       presence_dot_format,\n",
    "                       presence_dunder_name,\n",
    "                       presence_dunder_init,\n",
    "                       presence_def_colon,\n",
    "                       \n",
    "                       )\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(lang_data, lang_results)\n",
    "\n",
    "pipe = make_pipeline(lang_featurizer, DecisionTreeClassifier())\n",
    "# pipe = make_pipeline(lang_featurizer, MultinomialNB())\n",
    "pipe.fit(X_train, y_train)\n",
    "print('R^2 score: {}\\n'.format(pipe.score(X_test, y_test)))\n",
    "print(classification_report(pipe.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to test with the assignment's tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.09375\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Clojure       0.25      0.20      0.22         5\n",
      "    Haskell       0.00      0.00      0.00         0\n",
      "       Java       0.00      0.00      0.00         0\n",
      " JavaScript       0.00      0.00      0.00         2\n",
      "      OCaml       0.00      0.00      0.00         2\n",
      "        PHP       0.33      0.09      0.14        11\n",
      "     Python       0.25      0.25      0.25         4\n",
      "       Ruby       0.00      0.00      0.00         3\n",
      "      Scala       0.00      0.00      0.00         4\n",
      "     Scheme       0.00      0.00      0.00         1\n",
      "        TCL       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.18      0.09      0.12        32\n",
      "\n",
      "[[1 1 0 1 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 0 2 1 1 1 0 1 1 2 1]\n",
      " [0 0 0 1 0 2 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 3 0 0 0 0]\n",
      " [1 2 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahartz1/TIY/programming-language-classifier/.direnv/python-3.4.3/lib/python3.4/site-packages/sklearn/metrics/classification.py:960: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('R^2 score: {}\\n'.format(pipe.score(X_assignment_test, y_assignment_test)))\n",
    "print(classification_report(pipe.predict(X_assignment_test), y_assignment_test))\n",
    "print(confusion_matrix(pipe.predict(X_assignment_test), y_assignment_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
